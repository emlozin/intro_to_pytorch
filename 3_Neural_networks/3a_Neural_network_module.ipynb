{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3a. Neural networks with PyTorch using nn.Module\n",
    "\n",
    "PyTorch nn module offers building blocks (layers, functions, containers) to build neural networks. These are well presented in [torch.nn documentation](https://pytorch.org/docs/stable/nn.html \"torch.nn module documentation\"). This notebook should give you a good outlook of the module capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "\n",
    "import helper\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a neural network with tensors only would require a lot of work. To make this process easier, deep learning frameworks offer built-in building blocks for various architectures. PyTorch is not an exception.\n",
    "**torch.nn** module offers linear, convolutional, recurrent and many more types of layers that you can use in your future projects. \n",
    "\n",
    "In this notebook we will focus on linear and dropout layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear layers\n",
    "\n",
    "Linear layers are the layers which apply linear transformation to the input: \n",
    "<img src=\"https://latex.codecogs.com/gif.latex?y=xA^{T}&plus;b\" title=\"y=xA^{T}+b\" />\n",
    "\n",
    "In order to create a linear layer in PyTorch, you need to determine:\n",
    "\n",
    " - **in_features** - input size,\n",
    " - **out_features** - output size,\n",
    " - **bias** - specify whether bias is added to the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = torch.nn.Linear(in_features=100, out_features=20)\n",
    "print(linear_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout layers\n",
    "\n",
    "Dropout layers are used to prevent overfitting and randomly zeroes some of the elements coming from its input. This has been proved to be effective.\n",
    "\n",
    "In PyTorch dropout layers are defined as follows:\n",
    "\n",
    "- **p** - probability of zeroing element,\n",
    "- **inplace** - specify whether operation should be performed in-place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_layer = torch.nn.Dropout(p = 0.3)\n",
    "print(dropout_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "Activation functions are mathematical equations which determine whether neurons should get activated. They help with normalizing outputs of the neurons.\n",
    "\n",
    "These functions need to be efficient since they are called for every neuron - often thousands or millions of times. At the same time they should not lead to vanishing or exploding gradients.\n",
    "\n",
    "Some of the most popular activation functions are tanh, sigmoid and ReLU, presented below. For more information about their properties, refer to [7 types of Activation Function article](https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanh | Sigmoid | ReLU\n",
    ":--: | :-----: | :--:\n",
    "<img src=\"https://pytorch.org/docs/stable/_images/Tanh.png\"/> | <img src=\"https://pytorch.org/docs/stable/_images/Sigmoid.png\"/> | <img src=\"https://pytorch.org/docs/stable/_images/ReLU.png\"/>\n",
    "\n",
    "<p style='text-align: right;'> source: <a href=\"https://pytorch.org/docs/\">PyTorch docs</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch activation functions\n",
    "\n",
    "Thankfully, one does not have to implement activation functions on their own. PyTorch and other frameworks offer built-in activation function in their modules.\n",
    "\n",
    "This section will show how to use most popular ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input tensor\n",
    "input_tensor = torch.arange(-10.0,11.0)\n",
    "print(input_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions can be defined as layers with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate tanh output\n",
    "tanh_activation = nn.Tanh()\n",
    "tanh_out = tanh_activation(input_tensor)\n",
    "print(tanh_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or applied directly like regular functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate tanh output\n",
    "tanh_out = torch.tanh(input_tensor)\n",
    "print(tanh_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sigmoid output\n",
    "sigmoid_out = torch.sigmoid(input_tensor)\n",
    "print(sigmoid_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ReLU output\n",
    "relu_out = torch.relu(input_tensor)\n",
    "print(relu_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plots = {'Tanh': tanh_out.numpy(), 'Sigmoid': sigmoid_out.numpy(), 'ReLU': relu_out.numpy()}\n",
    "helper.plot_multiple(input_tensor.numpy(), plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "Load MNIST data and read sizes of training data and labels. You can see how the training data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from workshop import data\n",
    "\n",
    "# Transforms define which steps will be applied to each sample\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5],\n",
    "                         std=[0.5]),\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST(data.DATA_PATH, download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "helper.view_data(trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:\n",
    "\n",
    "Define a network with a following configuration:\n",
    "\n",
    "* one hidden linear layer with sigmoid activation function\n",
    "* one output linear layer with softmax activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Define hidden layer\n",
    "                \n",
    "        # TODO: Define output layer\n",
    "\n",
    "        # TODO: Define activation functions\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.hidden_activation(x)\n",
    "        x = self.output(x)\n",
    "        x = self.output_activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()\n",
    "helper.test_network(network, trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2:\n",
    "\n",
    "Define a network with an architecture of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Define all network layers \n",
    "        # Remember to define at least:\n",
    "        # - input to hidden (input size: )\n",
    "        # - hidden to output (output size: 10)\n",
    "        \n",
    "        # TODO: Define all activation functions\n",
    "        # - use softmax for last layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO: Pass input tensor through all defined layers\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()\n",
    "helper.test_network(network, trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "- [PyTorch NN module documentation](https://pytorch.org/docs/stable/nn.html)\n",
    "- [7 Types of Activation Functions: How to Choose?](https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro_to_pytorch",
   "language": "python",
   "name": "intro_to_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
