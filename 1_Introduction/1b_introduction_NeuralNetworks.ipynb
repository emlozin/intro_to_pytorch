{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1b. Introduction to Neural Networks\n",
    "\n",
    "Recommended reading: [Deep Learning](https://www.deeplearningbook.org/) by Ian Goodfellow, Yoshua Bengio and Aaron Courville"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biological Neuron\n",
    "\n",
    "Inspired by the biological neural network of the brain. Dentrites propagate the electrochemical signal to the cell body of the neuron. It is then decided whether to further transmit a signal to connected neurons via the axon terminal.\n",
    "\n",
    "\n",
    "<img src=\"../img/introduction/1920px-Neuron_Hand-tuned.svg.png\" width=\"700\" alt=\"Dentrites\">\n",
    "\n",
    "By Quasar Jarosz at English Wikipedia, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=7616130"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neuron\n",
    "\n",
    "### Perceptron\n",
    "\n",
    "The perceptron was invented in 1958 by Frank Rosenblatt along with a training algorithm.\n",
    "\n",
    "<img src=\"../img/introduction/Rosenblatt_21.jpg\" width=\"200\" alt=\"Frank Rosenblatt\">\n",
    "\n",
    "By Anonymous at English Wikipedia, CC BY-SA 4.0, https://commons.wikimedia.org/wiki/File:Rosenblatt_21.jpg\n",
    "\n",
    "Binary classifier function:\n",
    "\n",
    "$ f(x) = \\begin{cases} 1 & \\text{if } w_1x_1 + w_2x_2 + ... + w_nx_n > b\\\\ 0 & \\text{otherwise} \\end{cases} $\n",
    "\n",
    "The bias can also be represented as a weight. When we define a feature $ x_0 $ to be 1, then the first weight $ w_0 $ is a constant.\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "Let $ y_{pred} $ be the prediction and $ y_{true} $ be the desired output (label). The magnitude of updates is controlled by the learning rate $ r $.\n",
    "\n",
    "1. Initialize weights to 0 or small random value\n",
    "2. Calculate the predicted output $ y_{pred} = f(w*x) $\n",
    "3. Update weights $ w_i(t+1) = w_i(t) + r*(y_{true} - y_{pred}(t))*x_i $\n",
    "\n",
    "Caveats:\n",
    "* Works only for single-layer perceptrons, not for multi-layer perceptrons\n",
    "* Convergence only guaranteed for linear separable problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear separability\n",
    "\n",
    "<img src=\"../img/introduction/Linearly_separable_red-blue_cropped_.svg\" width=\"200\" alt=\"Linear Separability\">\n",
    "\n",
    "By Krb19 at English Wikipedia, CC BY-SA 4.0, https://commons.wikimedia.org/wiki/File:Linearly_separable_red-blue_cropped_.svg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "What we want:\n",
    "* Approximation of any continuous function on a compact subset\n",
    "* Easy training algorithm\n",
    "* Converging into the global optimum for convex problems, finding good local minima for non-convex problems\n",
    "\n",
    "### Universal approximation theorem\n",
    "\n",
    "A neural network with only one hidden layer can approximate any continuous function on a compact subset.\n",
    "\n",
    "Instead of a threshold function we use the sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "\n",
    "def plot_fn(fn, low, high, step=0.01):\n",
    "    xs = [x for x in np.arange(low, high, step)]\n",
    "    ys = list(map(fn, xs))\n",
    "    return plt.plot(xs, ys)\n",
    "\n",
    "\n",
    "plot_fn(sigmoid, -10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Derivative of sigmoid easy to compute $ \\sigma(x)` = \\sigma(x)*(1-\\sigma(x)) $, paper and pencil solution, see e.g. https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e\n",
    "2. Sigmoid can be made nearly identical to step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def like_step_function(w, b):\n",
    "    return lambda x: sigmoid(w*x + b)\n",
    "\n",
    "\n",
    "plot_fn(like_step_function(0.5, 0), -10, 10)\n",
    "plot_fn(like_step_function(2, 0), -10, 10)\n",
    "plot_fn(like_step_function(50, 0), -10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Step can be shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shifted_step(shift, w=50):\n",
    "    b = -shift*w\n",
    "    return lambda x: sigmoid(w*x + b)\n",
    "\n",
    "\n",
    "plot_fn(shifted_step(-5), -10, 0)\n",
    "plot_fn(shifted_step(5), 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Step can be scaled when passing result into output neuron with identity activation (no activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_step(height):\n",
    "    w = height\n",
    "    b = 0\n",
    "    \n",
    "    hidden = lambda x: sigmoid(50*x)\n",
    "    return lambda x: w*hidden(x) + b\n",
    "\n",
    "\n",
    "plot_fn(scaled_step(0.1), -10, 10)\n",
    "plot_fn(scaled_step(0.1), -10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Shift and scale can be combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_shifted_step(height, shift):\n",
    "    hidden = shifted_step(shift)\n",
    "    return lambda x: height*hidden(x)\n",
    "\n",
    "\n",
    "plot_fn(scaled_shifted_step(2, 1), 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Steps can be combined to build plateaus when using matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_shifted_steps(heights, shifts):\n",
    "    hidden = shifted_step(shifts)\n",
    "    return lambda x: np.matmul(heights, hidden(x))\n",
    "\n",
    "\n",
    "def combined_steps(heights, shifts):\n",
    "    heights = np.array(heights)\n",
    "    shifts = np.array(shifts)\n",
    "    \n",
    "    return scaled_shifted_steps(heights, shifts)\n",
    "    \n",
    "plot_fn(combined_steps([1, -1], [1, 2]), 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fn(combined_steps([1, -1, 2, -2, 0.75], [1, 2, 2, 3, 3]), 0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plateaus can be combined to approximate any continuous function on a compact subset. More neurons in the hidden layer make the approximation more fine-grained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer perceptron\n",
    "\n",
    "Has at least one hidden layer, all layers are fully connected. \n",
    "\n",
    "As a rule of thumb deeper with fewer neurons in each layer are computationally more efficient than shallow networks with more neurons in each layer. Very deep networks need techniques like skip connections to keep the optimization landscape less chaotic.\n",
    "\n",
    "The number of neurons in the layer greatly affects the number of multiply-add operations in the network and has a huge impact on computational resources required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_uniform_with_max(max_value):\n",
    "    return lambda inputs, num_outputs: np.random.rand(inputs, num_outputs) * max_value\n",
    "\n",
    "def layer(inputs, num_neurons, initializer=random_uniform_with_max(0.1), activation=sigmoid):\n",
    "    inputs = np.array(inputs)\n",
    "    weights = initializer(inputs.shape[-1], num_neurons)\n",
    "    return activation(np.matmul(inputs, weights))\n",
    "\n",
    "\n",
    "# inputs\n",
    "instance1 = [1, 2, 3]\n",
    "instance2 = [2, 5, 4]\n",
    "batch = [instance1, instance2]\n",
    "\n",
    "# network with 2 hidden layers\n",
    "fc1 = layer(batch, 64)\n",
    "fc2 = layer(fc1, 32)\n",
    "out = layer(fc2, 5)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Neural networks consist of differentiable operations. The common training method nowadays is gradient descent. The learning rate influences the step size into the downward direction. When the learning rate is too high, you may overshoot the minimum. When the learning rate is too low, training will take very long. Libraries like fast.ai offer a learning rate finder for a good learning rate to start with.\n",
    "\n",
    "<img src=\"../img/introduction/LearningRateTooLarge.svg\" width=700 alt=\"Learning Rate\">\n",
    "\n",
    "Image by Google at https://developers.google.com/machine-learning/crash-course/reducing-loss/learning-rate\n",
    "\n",
    "Convex problems like the logistic regression always converge into the global minimum. For non-convex problems the algorithm may find a local minimum which is hopefully good enough.\n",
    "\n",
    "<img src=\"../img/introduction/Non-Convex_Objective_Function.gif\" width=\"700\" alt=\"Optimization Landscape\">\n",
    "\n",
    "By Zachary kaplan at English Wikipedia, CC BY-SA 4.0, https://commons.wikimedia.org/wiki/File:Non-Convex_Objective_Function.gif\n",
    "\n",
    "Learn more about gradient descent at [Intro to optimization in deep learning: Gradient Descent](https://blog.paperspace.com/intro-to-optimization-in-deep-learning-gradient-descent/).\n",
    "\n",
    "Loss landscapes for various network architectures:\n",
    "\n",
    "<img src=\"../img/introduction/loss_landscapes.png\" width=\"700\" alt=\"Loss Landscapes\">\n",
    "\n",
    "From [Visualizing the Loss Landscape of Neural Nets](https://www.cs.umd.edu/~tomg/projects/landscapes/)\n",
    "\n",
    "For more info about advanced training methods, see e.g. [Snapshot Ensembles](https://arxiv.org/abs/1704.00109):\n",
    "\n",
    "<img src=\"../img/introduction/snapshot_ensemble.png\" width=\"700\" alt=\"Snapshot Ensemble\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization and Activation\n",
    "\n",
    "Initialization of weights needs to be chosen according to the activation function.\n",
    "\n",
    "When using the sigmoid function, large positive or negative weights are bad because the function saturates on both ends and the derivative will be near zero which in turn hinders backpropagation. Remember that the chain rule is applied for nested functions and multiplying small values with small values results in even smaller ones. You also have to consider the number of inputs in weight scaling because all of the inputs are multiplied by the weights and added up before they are passed into the sigmoid.\n",
    "\n",
    "As artificial neural networks were inspired by biology, sigmoid as a thresholding mechanism seemed like a natural choice. But saturation on both ends actually makes it perform badly in the backpropagation phase.\n",
    "\n",
    "ReLU (rectified linear units) solve this problem by activating linearly on the positive side. But because they activate to 0 on the negative side, the network can suffer from the dying ReLU problem when all neurons output 0 which also kills the back propagation signal. But computation is verfy efficient, even on mobile devices.\n",
    "\n",
    "To solve the problem of dying ReLUs, variants like the Leaky ReLU, PReLu, ELU, SELU have been introduced. They have a small slope on the negative side and so signal back propagation can recover.\n",
    "\n",
    "Which initialization method works well with which activation function is normally described in academic papers. Some common settings are described in Hands-On Machine Learning with Scikit-Learn and TensorFlow, 2nd edition, p. 328, table 11-1:\n",
    "\n",
    "| Initialization | Activation | \n",
    "| --- | --- | \n",
    "| Xavier Glorot | none, tanh, sigmoid, softmax |\n",
    "| He | ReLU & variants |\n",
    "| LeCun | SELU |\n",
    "\n",
    "Find out more about activation functions at https://mlfromscratch.com/activation-functions-explained/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots of Activation Functions\n",
    "\n",
    "The functions are documented at https://pytorch.org/docs/stable/nn.functional.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import relu, relu6, leaky_relu, tanh, prelu, elu, gelu\n",
    "from torch.nn import Identity\n",
    "from torch import tensor, sigmoid\n",
    "\n",
    "def plot_torch_fn(fn, low, high, step=0.01, plot=None, title=None):\n",
    "    xs = [tensor(x) for x in np.arange(low, high, step)]\n",
    "    ys = list(map(fn, xs))\n",
    "    \n",
    "    plot = plt if plot is None else plot\n",
    "    if title is not None:\n",
    "        plot.title.set_text(title)\n",
    "\n",
    "    return plot.plot(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [16, 12]\n",
    "fig, axs = plt.subplots(nrows=3, ncols=4)\n",
    "\n",
    "plot_torch_fn(Identity(), -10, 10, plot=axs[0, 0], title=\"Identity\")\n",
    "plot_torch_fn(sigmoid, -10, 10, plot=axs[0, 1], title=\"Sigmoid\")\n",
    "plot_torch_fn(tanh, -10, 10, plot=axs[0, 2], title=\"tanh\")\n",
    "\n",
    "plot_torch_fn(relu, -10, 10, plot=axs[1, 0], title=\"ReLU\")\n",
    "plot_torch_fn(relu6, -10, 10, plot=axs[1, 1], title=\"ReLU6\")\n",
    "plot_torch_fn(leaky_relu, -10, 10, plot=axs[1, 2], title=\"LeakyReLU\")\n",
    "plot_torch_fn(lambda x: prelu(x, weight=tensor(0.2, dtype=torch.double)), -10, 10, plot=axs[1, 3], title=\"PReLU\")\n",
    "\n",
    "plot_torch_fn(elu, -10, 10, plot=axs[2, 0], title=\"ELU\")\n",
    "plot_torch_fn(gelu, -10, 10, plot=axs[2, 1], title=\"GELU\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro_to_pytorch",
   "language": "python",
   "name": "intro_to_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
