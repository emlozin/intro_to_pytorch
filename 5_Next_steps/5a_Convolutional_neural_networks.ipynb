{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5a. Convolutional neural networks\n",
    "\n",
    "A traditional multilayered perceptron has many limitations. For exxample, it does not take spatial structure of data into consideration. Let's not forget about the fact that fully connected layers lead to huge numbers of weights for images with high resolution and therefore make it impossible to process data efficiently. \n",
    "\n",
    "That's where CNNs (convolutional neural networks) come into play. The idea behind them is inspired by how the visual cortex analyzes images. It creates and adapts filters that extract features. In contrast to classical image classifiers, the filters are not hand-engineered by experts but trained automatically within the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "\n",
    "import helper\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR10\n",
    "\n",
    "In this notebook, we will be working with the CIFAR10 dataset available in `torchvision`. It provides images of the objects of ten classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workshop import data\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "train_cifar = datasets.CIFAR10(data.DATA_PATH, download=True, train=True, transform=transform)\n",
    "test_cifar = datasets.CIFAR10(data.DATA_PATH, download=True, train=False, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_cifar, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_cifar, batch_size=64, shuffle=True)\n",
    "\n",
    "helper.view_data(trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN structure\n",
    "\n",
    "Let's take a look at one of the built-in CNN model available in the `torchvision` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "alexnet = models.alexnet()\n",
    "alexnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it contains many layers of different types. It may seem complicated at first, but those building blocks will be explained in this section - so don't worry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional layers\n",
    "\n",
    "It is no surprise that convolutional layers are the core concept used in CNNs. The convolution is an operation that can be seen as applying filters to the images.\n",
    "\n",
    "During model training, the convolutional layers 'learn' appropriate values for each filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolutional_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=0, stride=1)\n",
    "print(convolutional_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "\n",
    "Create a convolutional layer (conv2d) that is equivalent to applying a [Sobel filter](https://en.wikipedia.org/wiki/Sobel_operator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected results\n",
    "helper.view_filters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a tensor of size 1x3x3x3 with Sobel X kernel as values\n",
    "sobel_x = \n",
    "# TODO: Create a convolutional layer with RGB image as input and output and no bias\n",
    "sobel_layer_x = \n",
    "# TODO: Initialize weights with Sobel kernel data\n",
    "sobel_layer_x.weight.data = \n",
    "\n",
    "pd.DataFrame(data=sobel_x.squeeze().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a tensor of size 1x3x3x3 with Sobel Y kernel as values\n",
    "sobel_y = \n",
    "# TODO: Create a convolutional layer with 3RGB image as input and output and no bias\n",
    "sobel_layer_y = \n",
    "# TODO: Initialize weights with Sobel kernel data\n",
    "sobel_layer_y.weight.data = \n",
    "\n",
    "pd.DataFrame(data=sobel_y.squeeze().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = {\n",
    "    'Sobel X': (lambda img: sobel_layer_x(img)),\n",
    "    'Sobel Y': (lambda img: sobel_layer_y(img))\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    helper.view_filters(filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling layers\n",
    "\n",
    "Using convolutional layers can lead to increasing dimension of the layers. In order to reduce the number of parameters in the network, we use pooling layers. These are the layers which down-sample their inputs by using a selected function (eg. maximum value within the frame). This process is presented in the picture below: \n",
    "\n",
    "![pooling layer](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Max_pooling.png/314px-Max_pooling.png \"Max pooling layer\") \n",
    "source: [wikipedia.org](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Max_pooling.png/314px-Max_pooling.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_array = np.arange(-8, 8, dtype=np.float32).reshape((1,4,4))\n",
    "input_layer = torch.from_numpy(input_array)\n",
    "\n",
    "pd.DataFrame(data=input_layer.numpy().reshape((4,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max\n",
    "\n",
    "One of the most popular types of pooling layers is max-pooling. As its name suggests, it takes the highest value within the kernel. In PyTorch, there are several variants of max-pooling:\n",
    "\n",
    "- `nn.MaxPool2d` - simplest to use: you specify the size of the kernel, stride and padding yourself,\n",
    "- `nn.AdaptiveMaxPool2d` - you specify the size of the desired output, size of the kernel - other parameters are adapted according to the given parameters,\n",
    "- `nn.FractionalMaxPool2d` - applies fractional max pooling: you specify the size of the kernel and the size of the output/output ratio, described in detail in the paper [Fractional MaxPooling by Ben Graham](https://arxiv.org/abs/1412.6071)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2:\n",
    "\n",
    "Declare a max-pooling layer, pass the input layer through it and display results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pool = \n",
    "pd.DataFrame(data=max_pool(input_layer).numpy().reshape((2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adp_max_pool = \n",
    "pd.DataFrame(data=adp_max_pool(input_layer).numpy().reshape((2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_max_pool = \n",
    "pd.DataFrame(data=frac_max_pool(input_layer).numpy().reshape((2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average\n",
    "\n",
    "Another type of pooling layer is average-pooling. Again, PyTorch provides several options to use it:\n",
    "\n",
    "- `nn.AvgPool2d` - simplest to use: you specify the size of the kernel, stride and padding yourself,\n",
    "- `nn.AdaptiveAvgPool2d` - you specify the size of the output, size of the kernel, other parameters are adapted according to the given parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3:\n",
    "\n",
    "Declare an average-pooling layer, pass the input layer through it and display results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_pool = \n",
    "pd.DataFrame(data=avg_pool(input_layer).numpy().reshape((2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adp_avg_pool = \n",
    "pd.DataFrame(data=adp_avg_pool(input_layer).numpy().reshape((2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power average\n",
    "\n",
    "The last type of built-in pooling in PyTorch is power-average-pooling, which calculates the output according to\n",
    "\n",
    "\\begin{equation}\n",
    "f(X)=\\sqrt[p]{\\sum_{x \\in X}x^p}\n",
    "\\end{equation}\n",
    "\n",
    "If you use $p=1$, you get sum pooling. In the limit of $p \\rightarrow \\infty $, it converges to max-pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4:\n",
    "\n",
    "Declare a power-average-pooling layer, pass the input layer through it and display results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_pool = \n",
    "pd.DataFrame(data=lp_pool(input_layer).numpy().reshape((2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation layers\n",
    "\n",
    "Another block in CNNs are activation layers which were discussed in [Neural networks notebook](../3_Neural_networks/3a_Neural_network_module.ipynb). If you need to refresh your memory, please refer to the section Activation layers there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5:\n",
    "\n",
    "Declare a ReLU activation layer and pass the output of the pooling layer of your choice through it. Display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_layer = \n",
    "pd.DataFrame(data=act_layer(adp_avg_pool(input_layer)).numpy().reshape((2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier\n",
    "\n",
    "The classifier is the last part of the network. It is a linear neural network which analyzes features extracted by the previous blocks and provides information about the predicted class. Again, for more about linear networks, please refer to [Neural networks notebook](../3_Neural_networks/3a_Neural_network_module.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6:\n",
    "\n",
    "Declare classifier similar to the one used in AlexNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = \n",
    "classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "\n",
    "We discussed the basics of CNNs. Now it's time to implement your first network. Create network with:\n",
    "\n",
    "- 3 convolutional layers, each followed by:\n",
    "  * activation layer,\n",
    "  * pooling layer,\n",
    "  \n",
    "- classifier with 2 linear layers, each:\n",
    "  * preceeded with dropout layer,\n",
    "  * followed by activation layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # input ?x?x?\n",
    "        self.conv_1 = \n",
    "        # input ?x?x?\n",
    "        self.conv_2 = \n",
    "        # input ?x?x?\n",
    "        self.conv_3 = \n",
    "        \n",
    "        self.pool = \n",
    "        \n",
    "        # input 4x4x64\n",
    "        self.lin_1 = \n",
    "        self.lin_2 =\n",
    "        \n",
    "        self.dropout = \n",
    "        self.act = \n",
    "        self.class_act = \n",
    "        \n",
    "    def forward(self, x):        \n",
    "        x = \n",
    "        \n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "        \n",
    "        x = \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()\n",
    "network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All methods required to test your network have already been implemented in [Training and validation notebook](../4_Training_and_validation/4c_training_loop_Solutions.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "train_losses, test_losses, accuracies = helper.train_nn(network, loss, trainloader, testloader, n_epochs=10, lr=0.5)\n",
    "helper.plot_metrics(train_losses, test_losses, accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    helper.view_classify(trainloader, network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [PyTorch NN module documentation](https://pytorch.org/docs/stable/nn.html)\n",
    "- [Convolutional neural network](https://en.wikipedia.org/wiki/Convolutional_neural_network)\n",
    "- [Convolutional layers for deep learning neural networks](https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro_to_pytorch",
   "language": "python",
   "name": "intro_to_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
