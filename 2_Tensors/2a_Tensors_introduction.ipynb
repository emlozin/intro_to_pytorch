{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Introduction to tensors\n",
    "\n",
    "Free after [Deep Learning with PyTorch, Eli Stevens, Luca Antiga, and Thomas Viehmann](https://www.manning.com/books/deep-learning-with-pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<style>\n",
    "th {\n",
    "  font-size: 24px\n",
    "}\n",
    "td {\n",
    "  font-size: 16px\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import helper\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"ticks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core concepts of this section\n",
    "\n",
    "1. A `Tensor` is a `View` onto a `Storage`\n",
    "2. `contiguous` memory layout enables fast computations\n",
    "3. `broadcasting`: expand Tensor dimensions as needed\n",
    "\n",
    "## Fundamentals\n",
    "### Contrast to python list\n",
    "\n",
    "<!-- ![](../img/memory.png \"src: \") -->\n",
    "<div align=\"center\">\n",
    "    <img src=\"../img/memory.svg\" width=\"1200px\" alt=\"in pytorch, a tensor refers to numbers in memory that are all next to each other\">\n",
    "</div>\n",
    "\n",
    "    \n",
    "| entity | plain python | pytorch| \n",
    "|:-------|:------------:|:------:|\n",
    "| numbers | **boxed**: objects with reference counting | 32 bit numbers| \n",
    "| lists | sequential (1dim) collections of pointers to python objects | **adjacent entries in memory**: optimized for computational operations | \n",
    "| interpreter | slow list and math operations | fast | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiation\n",
    "\n",
    "Default type at instantiation is torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(3); print(a, a.dtype)\n",
    "b = torch.zeros((3, 2)).short(); print(b)\n",
    "c = torch.tensor([1.,2.,3.], dtype=torch.double); print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors and storages\n",
    "\n",
    "* the `torch.Storage` is where the numbers actually are\n",
    "* A `torch.Tensor` is a view onto a *torch.Storage*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1,2,3,4,5,6])\n",
    "b = a.reshape((3,2))\n",
    "assert id(a.storage()) == id(b.storage())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* layout of the storage is always *1D*\n",
    "* hence, changing the value in the storage changes the values of all views (i.e. torch.Tensor) that refer to the same storage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size, storage offset, and strides\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"../img/tensor.svg\" width=\"1200px\" alt=\"Meaning of size, offset and stride\">\n",
    "</div>\n",
    "\n",
    "* A Tensor is a view on a storage that is defined by its\n",
    "  * **size:** `t.size()` / `t.shape`\n",
    "  * **storage offset:** `t.stoage_offset()`\n",
    "  * **stride:** `t.stride()`\n",
    "* the **stride** informs how many elements in the storage one needs to move to get to the next value in that dimension\n",
    "* to get `t[i,j]`, get `storage_offset + i * stride[0] + j * stride[1]` of storage\n",
    "* this makes some tensor operations very cheap, because a new tensor has the same storage but different values for size, offset and stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2,3], [4,5,6]])\n",
    "print(f\"a.size: {a.size()}\")\n",
    "print(f\"a.storage_offset: {a.storage_offset()}\")\n",
    "print(f\"a.stride: {a.stride()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[1]\n",
    "print(f\"b.size: {b.size()}\")\n",
    "print(f\"b.storage_offset: {b.storage_offset()}\")\n",
    "print(f\"b.stride: {b.stride()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transposing a tensor\n",
    "\n",
    "* the transpose just swaps entries in size and stride\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"../img/transpose.svg\" width=\"1200px\" alt=\"Transpose explained\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contiguous\n",
    "\n",
    "* A tensor whose values are laid out in the storage starting from the right most dimension onward is **contiguous**\n",
    "  * e.g. 2D tensor:\n",
    "    * `t.size() # torch.Size([#rows, #columns])`\n",
    "    * moving along rows (i.e. fix row, go from one column to the next) is equivalent to going through storage one by one\n",
    "* this data locality improves performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2,3], [4,5,6]])\n",
    "assert a.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.t()\n",
    "assert not b.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = b.contiguous()\n",
    "assert c.is_contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric types\n",
    "\n",
    "* `torch.floatXX`: 32: float, 64: double, 16: half\n",
    "* `torch.intXX`: 8, 16, 32, 64\n",
    "* `torch.uint8`: torch.ByteTensor\n",
    "* `torch.Tensor`: equivalent to torch.FloatTensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:\n",
    "\n",
    "Create a tensor `a` from `list(range(9))`. Predict then check what the size, offset, and strides are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import test_attributes\n",
    "\n",
    "# TODO define tensor\n",
    "a = None\n",
    "test_attributes(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2:\n",
    "\n",
    "Create a tensor `b = a.view(3, 3)`. What is the value of `b[1,1]`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO define tensor\n",
    "b = None \n",
    "\n",
    "b[1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3:\n",
    "\n",
    "Create a tensor `c = b[1:,1:]`. Predict then check what the size, offset, and strides are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO define tensor\n",
    "c = None\n",
    "\n",
    "test_attributes(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing and Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "* similar to [numpy indexing](https://numpy.org/devdocs/user/basics.indexing.html), e.g. `points[1:, 0]`: all but first rows, first column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tips and tricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise indexing works\n",
    "t = torch.tensor(range(1, 10)).reshape(3, -1)\n",
    "diagonal = t[range(3), range(3)]\n",
    "diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inject additional dimensions with indexing\n",
    "\n",
    "t = torch.rand((3, 64, 64))\n",
    "\n",
    "# Index with `None` at second dim to `unsqeeze`.\n",
    "assert t[:, None].shape == torch.Size([3, 1, 64, 64])\n",
    "\n",
    "# Do it multiple times\n",
    "assert t[:, None, : , None].shape == torch.Size([3, 1, 64, 1, 64])\n",
    "\n",
    "# Can also use ellipsis\n",
    "assert t[..., None].shape == torch.Size([3, 64, 64, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: \n",
    "\n",
    "Get the diagonal elements of `t.rand(3, 3)` by reshaping into a 1d tensor and taking every fourth element, starting from the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand(3,3)\n",
    "# TODO: Calculate actual tensor\n",
    "diag_actual = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import test_indexing\n",
    "test_indexing(t, diag_actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting\n",
    "\n",
    "Look at the examples below and think about why we can multiply two tensors of different shapes and get the result that one would expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([\n",
    "    3\n",
    "])\n",
    "b = torch.tensor([\n",
    "    1, 2, 3\n",
    "])\n",
    "torch.allclose(a*b, torch.tensor([\n",
    "    3, 6, 9\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([\n",
    "    [1, 2],\n",
    "    [3, 4]\n",
    "])\n",
    "b = torch.tensor([\n",
    "    1, 2\n",
    "])\n",
    "torch.allclose(a*b, torch.tensor([\n",
    "    [1, 4],\n",
    "    [3, 8]\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is that PyTorch magically *expands* the shape of the tensors in a smart way such that operations can be performed.\n",
    "&rarr; This is called **broadcasting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is broadcasting done?\n",
    "\n",
    "1. Compare the dimensions of all tensors, starting from the trailing one.\n",
    "2. If dims are the same, do nothing\n",
    "3. If one dim is 1 (or missing), expand it to match the other dim.\n",
    "4. Else: abort\n",
    "\n",
    "**Note:** When broadcasting, PyTorch does not actually need to expand the dimensions of a tensor in memory in order to perform efficient tensor operations.\n",
    "\n",
    "```\n",
    "Example 1\n",
    "[a]:    3 x 64 x 64\n",
    "[b]:              1\n",
    "[a*b]:  3 x 64 x 64\n",
    "\n",
    "Example 2\n",
    "[a]:    3 x  1 x 64\n",
    "[b]:    1 x 64 x  1\n",
    "[a*b]:  3 x 64 x 64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 - Broadcasting: \n",
    "\n",
    "Write down the shapes of the tensors in the examples and convince yourself that the output shape is as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define tensors from Example 1 and check output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define tensors from Example 2 and check output shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
