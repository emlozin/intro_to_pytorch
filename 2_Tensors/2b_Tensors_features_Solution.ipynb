{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2b. Tensor features\n",
    "\n",
    "Free after [Deep Learning with PyTorch, Eli Stevens, Luca Antiga, and Thomas Viehmann](https://www.manning.com/books/deep-learning-with-pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"ticks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([1,2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy interoperability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_np = t.numpy()\n",
    "t = torch.from_numpy(t_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialization\n",
    "\n",
    "* use `torch.save(t, \"path_to_file.t\")` and `torch.load(\"path_to_file.t\")`\n",
    "* alternatively, can use in combination with `hdf5` file format (library: h5py)\n",
    "\n",
    "### GPU\n",
    "\n",
    "* PyTorch makes it very easy to use one or several GPUs, using the `torch.device`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.device(\"cpu\") # use cpu by default\n",
    "torch.device(\"cuda\") # GPU\n",
    "torch.device(\"cuda:0\") # index multiple GPUs #0 -> default: 0\n",
    "torch.device(\"cuda:1\") # use GPU #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* delegate a tensor to a device using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.to(torch.device(\"cpu\")))\n",
    "print(t.cpu())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(t.cuda())\n",
    "    print(t.cuda(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor API\n",
    "\n",
    "For PyTorch, there exists a ton of ops... whatever you would like to do, it's probably already implemented in a performant manner.\n",
    "\n",
    "**PyTorch convention:** a mathematical operation often has an in-place equivalent referenced by using the suffix `_`. E.g. `t.cos()` and `t.cos_()`\n",
    "\n",
    "Some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor(range(10), dtype=torch.float) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.log_(); t # operates in-place/ mutates tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6:\n",
    "\n",
    "Calculate the mean squared error between predictions and target values: \n",
    "\n",
    "\n",
    "<center>$\\rm mse = \\frac{1}{N}\\sum_i^N (p_i - t_i)^2$</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(p, t):\n",
    "    return (p - t).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import test_mse\n",
    "test_mse(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Group Normalizatoin Paper](https://arxiv.org/pdf/1803.08494.pdf) shows a nice figure on how different normalization schemes slice a tensor.\n",
    "\n",
    "![Figure](../img/group_norm.png)\n",
    "\n",
    "Choose one scheme and normalize the below tensor accordingly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BATCH_SIZE, C_NUMBER_OF_CHANNELS, H_HEIGHT, W_WIDTH = 32, 3, 64, 64\n",
    "\n",
    "t = torch.rand(N_BATCH_SIZE, C_NUMBER_OF_CHANNELS, H_HEIGHT, W_WIDTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto grad\n",
    "\n",
    "Fundamental to optimization is the ability to perform differentiation. PyTorch does this with its **autograd** framework, which we will dive into now.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"../img/autograd.svg\" width=\"1200px\" alt=\"Tracking derivatives through the compute graph\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Concepts\n",
    "\n",
    "1. Compute graph and chain rule \n",
    "1. `t.requires_grad_()` and `t.grad`\n",
    "2. `t.backward()`\n",
    "3. `param.detach` and `torch.no_grad()` \n",
    "4. zeroing the gradient\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent by hand...\n",
    "\n",
    "We want to find the minimum of a quadratic function and show how PyTorch can help us to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a quadratic function and plot it\n",
    "\n",
    "def second_order_polynomial(x, a, b, c):\n",
    "    return a*x**2 + b*x + c\n",
    "\n",
    "def show_sop(x, y):\n",
    "    fig, ax = plt.subplots(1, figsize=(7,7))\n",
    "    ax.set_ylabel(\"$y$\", fontsize=20)\n",
    "    ax.set_xlabel(\"$x$\", fontsize=20)\n",
    "    ax.plot(x, y, linewidth=4 )\n",
    "    ax.set_title(\"$ax^2 + bx + c$\", fontsize=24)\n",
    "\n",
    "a, b, c = 0.5, 1.3, 2.8\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "\n",
    "show_sop(x, second_order_polynomial(x, a, b, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know that $\\frac{d}{dx} f(x) = \\frac{d}{dx} ax^2 + bx + c = 2ax + b$.\n",
    "\n",
    "Does PyTorch also know that? Let's see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Dependent variable\n",
    "\n",
    "We first need to let PyTorch know that $x$ is our dependent variable. We do so by specifying that $x$ requires the computation of gradients, using `requires_grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2.5], requires_grad=True)\n",
    "\n",
    "# or \n",
    "x = torch.tensor([2.5])\n",
    "x.requires_grad_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Perform computations with dependent variable \n",
    "\n",
    "Next, we want to compute something with this variable, namely our quadratic function $f(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = second_order_polynomial(x, a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the `requires_grad` attribute, PyTorch dynamically tracks the dependency on `x` on any computation on x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Compute the gradients\n",
    "\n",
    "Now we wish to compute the gradients. This is simply done by calling `backward()` on $y$. The gradients can then be found in the `x.grad` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.grad)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Check agreement\n",
    "\n",
    "Let's also check with the expected result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x.grad == 2*a*x + b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Repeat\n",
    "\n",
    "##### **Parameter update**\n",
    "\n",
    "* Notice that we haven't found a value of `x` yet where $f(x)$ is minimum.\n",
    "* But the gradient descent algorithm at least tells us in which direction we should continue our search.\n",
    "* Since the gradient is positive, we know that `f(x)` keeps growing in the postiive `x` direction. Hence, we should choose a smaller value for `x`.\n",
    "* However, if we now operate on `x` in order to reduce its value, we will change the graph of `x`. To avoid this, we can ask PyTorch to operate on `x` without tracking this operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x -= 1. # just guessed some value\n",
    "print(x.requires_grad) # still requires grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Side note:** It can sometimes be necessary to stop computing gradients altogether. In this case, use `x.detach`:\n",
    "\n",
    "```python\n",
    "some_other_thing = x.detach()\n",
    "assert not some_other_thing.requires_grad\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Zeroing the gradient**\n",
    "\n",
    "Notice that `x` still has a gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everytime we call `backward` on some `y(x)`, we will accumulate gradients in `x`. This is helpful if for example we want to compute gradients across multiple GPUs...\n",
    "\n",
    "But fow now that is not what we want to do. Instead we want to compute the gradient for a new value of `x`. So we whould reset `x.grad`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Next**\n",
    "Now, let's go back to step 2. And see if we are closer to the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "1. Use `requires_grad` to let PyTorch know your dependent variable.\n",
    "2. Now every operation on `x` is tracked in order to dynamically build the compute graph involving `x`.\n",
    "3. Use `y.backward()` to compute the gradient of `y` using the chain rule. This works because the Tensor framework implements a `forward` and `backward` operation for each computational operation. This includes overloading `a.__mult__(self, b)` etc.\n",
    "4. Make sure to `detach` some operations on `x` form the compute graph if they are not required for the computation of gradients. Use `x.detach` or `torch.no_grad()` \n",
    "5. Each call to `y.backward()` will accumulate gradients in the leaves of the graph. Make sure to zero the gradients after a parameter udpate.\n",
    "\n",
    "These are the essential steps to computing gradients with PyTorch. We will later discover PyTorch's higher-level API that helps us make those steps more user friendly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: Least squares fit for a linear function\n",
    "\n",
    "Find the best set of parameters `m, b` for a linear model $f(x) = mx + b$ that best fit the data.\n",
    "\n",
    "To do so, you will have to:\n",
    "1. Decide which are your dependent variables.\n",
    "2. Calculate the mean squared error.\n",
    "3. Calculate the gradient of the mse with respect to the dependent variables.\n",
    "4. Perform a parameter update.\n",
    "5. Iterate until some stopping condition.\n",
    "\n",
    "To help you with these task, some functions and the training loop are already set up for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import linear_model, noise, mse, show_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "initial_parmas = torch.tensor([1., 0.])\n",
    "target_params = torch.tensor([3.4, -0.8])\n",
    "\n",
    "x = torch.tensor(range(10))\n",
    "data = linear_model(x, target_params) + noise(x)\n",
    "\n",
    "assert mse(linear_model(x, initial_parmas), data) > 100\n",
    "\n",
    "show_fit(x, linear_model(x, initial_parmas), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training loop\n",
    "lr = 0.01\n",
    "n_epochs = 10\n",
    "initial_parmas = torch.tensor([1., 0.], requires_grad=True)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # calculate loss\n",
    "    loss = mse(linear_model(x, initial_parmas), data)\n",
    "    print(f\"Loss at epoch [{epoch}]: [{loss.item()}]\")\n",
    "    \n",
    "    # calculate gradients / propagate error\n",
    "    loss.backward()\n",
    "    \n",
    "    # update weights\n",
    "    with torch.no_grad():\n",
    "        initial_parmas -= lr * initial_parmas.grad.data\n",
    "        initial_parmas.grad.data.zero_()\n",
    "    \n",
    "with torch.no_grad():\n",
    "    show_fit(x, linear_model(x, initial_parmas), data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
