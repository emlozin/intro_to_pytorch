{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4d. Inference\n",
    "\n",
    "After having saved the model, we want to use it for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import collections\n",
    "import math\n",
    "from typing import Tuple, List\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "from workshop import data\n",
    "import helper\n",
    "\n",
    "import time\n",
    "\n",
    "def get_model():\n",
    "    return torch.nn.Sequential(collections.OrderedDict([\n",
    "        (\"reshape\", torch.nn.Flatten()),\n",
    "        (\"hidden\", torch.nn.Linear(28*28,256)),\n",
    "        (\"sigmoid\", torch.nn.Sigmoid()),\n",
    "        (\"output\", torch.nn.Linear(256,10)),\n",
    "      ]))\n",
    "\n",
    "\n",
    "def time_it(f, n, *args):\n",
    "    # warmup\n",
    "    f(*args)\n",
    "    \n",
    "    # measure\n",
    "    start = time.time()\n",
    "    for _ in range(n):\n",
    "        f(*args)\n",
    "    return (1000 * (time.time() - start)) / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading\n",
    "\n",
    "Load the whole model (architecture, parameters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (reshape): Flatten()\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(\"../data/model.pt\")\n",
    "model.eval()  # important because operations like dropout behave differently on inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only load the model's parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (reshape): Flatten()\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.load_state_dict(torch.load(\"../data/model_params.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing for inference\n",
    "\n",
    "PyTorch builds a dynamic graph. But for most models an optimized static graph can be saved.\n",
    "\n",
    "PyTorch models can be converted to TorchScript which makes the model more portable (e.g. you can also use it in a C++ program). TorchScript models are run by a faster interpreter.\n",
    "\n",
    "Tracing will invoke the model with example data and record the operations to build an optimized graph.\n",
    "\n",
    "For more information, see [TorchScript](https://pytorch.org/docs/stable/jit.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. inference time for normal model: 0.0820\n",
      "Avg. inference time for traced model: 0.0676\n"
     ]
    }
   ],
   "source": [
    "example_data = torch.rand(1, 784)\n",
    "\n",
    "traced_model = torch.jit.trace(model, (example_data,))\n",
    "\n",
    "time_normal = time_it(model, 200, example_data)\n",
    "print(f\"Avg. inference time for normal model: {time_normal:.4f}\")\n",
    "\n",
    "time_traced = time_it(traced_model, 200, example_data)\n",
    "print(f\"Avg. inference time for traced model: {time_traced:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 (Optional):\n",
    "\n",
    "If you have a cuda enabled graphics cards, send data and the model to the gpu by calling *.cuda()* and compare the inference time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro_to_pytorch",
   "language": "python",
   "name": "intro_to_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
