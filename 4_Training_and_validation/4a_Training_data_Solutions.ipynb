{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4a. Handling data with PyTorch\n",
    "\n",
    "We will now look at the `torch.utils.data` package and explore how PyTorch supports us in handling training and validation data.\n",
    "\n",
    "## Key concepts of this section\n",
    "\n",
    "1. `torch.utils.data.Dataset`\n",
    "2. `torch.utils.data.DataLoader`\n",
    "3. **Data transforms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "from intro_to_pytorch import data\n",
    "import helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset\n",
    "\n",
    "A [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) is an abstraction layer that handles reading data from specified source. This could mean reading images from the file system, strings from a NoSQL data base, ...\n",
    "\n",
    "In `PyTorch`, a (map-style) dataset is a class that implements the `__getitem__(self, key)` method, which returns $(x, y)$ and `__len__(self)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "For this set of exercises, you are given the raw data already (kindly provided by the `torchvision` library). To check other built-in datasets refer to [PyTorch documentation](https://pytorch.org/docs/stable/torchvision/datasets.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "Create your own PyTorch MNIST dataset.\n",
    "\n",
    "Assume that `x` is a tensor of `28x28` tensors (MNIST image pixels) and `y` is a tensor of labels (0-9) in the same order as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDataSet:\n",
    "    def __init__(self, x, y):\n",
    "        self.x, self.y = x, y\n",
    "    \n",
    "    def __getitem__(self, key) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.x[key], self.y[key]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "x_train, y_train = torch.load(data.DATA_PATH / \"MNIST/processed/training.pt\")\n",
    "\n",
    "train_mnist = MnistDataSet(x_train.float(), y_train)\n",
    "assert len(train_mnist) == 60000\n",
    "\n",
    "x, y = train_mnist[:4]\n",
    "\n",
    "torch.testing.assert_allclose(y, [5, 0, 4, 1])\n",
    "assert x.shape == (4, 28, 28)\n",
    "\n",
    "helper.show_batch_of_data(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2:\n",
    "Refactor your data set by moving the call to `torch.load` to the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDataSet:\n",
    "    def __init__(self, train=True):\n",
    "        subset = \"training\" if train else \"test\"\n",
    "        self.x, self.y = torch.load(data.DATA_PATH / f\"MNIST/processed/{subset}.pt\")\n",
    "        self.x = self.x.float()\n",
    "    \n",
    "    def __getitem__(self, key) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.x[key], self.y[key]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "train_mnist = MnistDataSet()\n",
    "assert len(train_mnist) == 60000\n",
    "\n",
    "x, y = train_mnist[:4]\n",
    "\n",
    "torch.testing.assert_allclose(y, [5, 0, 4, 1])\n",
    "assert x.shape == (4, 28, 28)\n",
    "\n",
    "helper.show_batch_of_data(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3:\n",
    "\n",
    "There really isn't much more to a dataset than this. Check the MNIST dataset as it is provided by the `torchvision` library and see if it does the same thing as ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mnist = torchvision.datasets.MNIST(data.DATA_PATH, download=True, train=True)\n",
    "\n",
    "# Answer: Does not handle slices and returns PIL.Image instead of tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data loader\n",
    "\n",
    "A dataset knows where input data resides and how to read it. But during training we want to do much more than just reading data, for example:\n",
    "\n",
    "1. Data batching\n",
    "1. Data augmentation\n",
    "1. Hard-example mining\n",
    "1. Lift data imbalance\n",
    "1. ...\n",
    "\n",
    "This is where [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) comes into play. PyTorch (and its application libraries) offers a wide range of data loaders that may be used to address the above points. Essentially though, they provide a means to iterate over a Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4:\n",
    "\n",
    "Implement an MNIST dataloader that returns random batches of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class MnistDataLoader:\n",
    "    def __init__(self, dataset, batch_size, shuffle):\n",
    "        self.dataset, self.batch_size, self.shuffle = dataset, batch_size, shuffle\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.idx = list(range(len(self.dataset)))\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.idx)\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.idx:\n",
    "            batch, self.idx = self.idx[:self.batch_size], self.idx[self.batch_size:]\n",
    "            return self.dataset[batch]\n",
    "        raise StopIteration()\n",
    "\n",
    "n_epochs = 2\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch #{epoch+1}\")\n",
    "    dl = MnistDataLoader(MnistDataSet(train=False), 4, True)\n",
    "    helper.show_first_n_batches(dl, n=2) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data transforms\n",
    "\n",
    "Beyond providing data in random batches, we can extend our data loader to perform transformations on $x$ and $y$. \n",
    "\n",
    "These transformation can be used for data preparation (like normalization) and augmentation.\n",
    "\n",
    "Because of the very simple tranformer api, it is very easy to create your own transformers or to throw in, say, your favourite image augmentation library, e.g. [albumentations](https://albumentations.ai/).\n",
    "\n",
    "Of course, the PyTorch application libraries already provide a great selection of transforms, e.g. for [images](https://pytorch.org/docs/stable/torchvision/transforms.html) or [audio](https://pytorch.org/audio/stable/transforms.html).\n",
    "\n",
    "Let's take a look at one of them, namely the [Compose transform](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Compose):\n",
    "\n",
    "- We notice that it has a simple implementation and it receives only $x$ as an argument.\n",
    "- Maybe this is enough for our use case. But in case we are dealing with segmentation masks, we will have to impose our own implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5:\n",
    "\n",
    "**Data normalization.** The tensors that come out of our data loader are still pixel values between 0.0 and 255.0.\n",
    "\n",
    "Calculate the data's `mean` and `std` and add a transform that rescales the images. Extend the `MnistDataLoader` to be able to use such a transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDataLoader:\n",
    "    def __init__(self, dataset, batch_size, shuffle, transform=None):\n",
    "        self.dataset, self.batch_size, self.shuffle, self.transform = dataset, batch_size, shuffle, transform\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.idx = list(range(len(self.dataset)))\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.idx)\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.idx:\n",
    "            batch, self.idx = self.idx[:self.batch_size], self.idx[self.batch_size:]\n",
    "            x, y = self.dataset[batch]\n",
    "            if self.transform:\n",
    "                return self.transform(x, y)\n",
    "            return x, y\n",
    "        \n",
    "        raise StopIteration()\n",
    "\n",
    "class ImageNormalizer:\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean, self.std = mean, std\n",
    "    \n",
    "    def __call__(self, x, y):\n",
    "        return (x - self.mean).div_(self.std), y\n",
    "\n",
    "mean = x_train.double().mean()\n",
    "std = x_train.double().std()\n",
    "\n",
    "print(f\"Global mean: {mean.item()}\")\n",
    "print(f\"Global std: {std.item()}\")\n",
    "\n",
    "dl = MnistDataLoader(MnistDataSet(train=False), 4, True, ImageNormalizer(mean, std))\n",
    "x, y = next(iter(dl))\n",
    "print(x.mean(), x.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) Exercise 6\n",
    "\n",
    "Write a transform that performs a horizontal flip for digits 0, 1, 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitFlipper:\n",
    "    def __init__(self, digits_to_flip: List[int], prob=0.5):\n",
    "        self.digits_to_flip, self.prob = digits_to_flip, prob\n",
    "        \n",
    "    def __call__(self, x, y):\n",
    "        for i, digit in enumerate(y.numpy()):\n",
    "            if digit in self.digits_to_flip and random.random() <= self.prob:\n",
    "                x[i] = x[i].flip(-1)\n",
    "        return x, y\n",
    "    \n",
    "\n",
    "n_epochs = 2\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch #{epoch+1}\")\n",
    "    dl = MnistDataLoader(MnistDataSet(train=False), 8, False, transform=DigitFlipper([0, 1, 8], 0.5))\n",
    "    helper.show_first_n_batches(dl, n=2) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro_to_pytorch",
   "language": "python",
   "name": "intro_to_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
