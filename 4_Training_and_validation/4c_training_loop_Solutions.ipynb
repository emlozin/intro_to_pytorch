{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4c. Improving the training loop\n",
    "\n",
    "Now that we are able to compute the loss for our training data, we are able to train the model with the same couple of steps that we have encountered at the end of [**Notebook 2**](../2_Tensors/2b_Tensors_features_Solution.ipynb).\n",
    "\n",
    "We will take this as a starting point to introduce the `torch.optim` package which provides us with the `Optimizer` API that greatly simplifies the training loop.\n",
    "\n",
    "## Key concepts of this section\n",
    "\n",
    "1. `Optimizer` API from the `torch.optim` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import collections\n",
    "import math\n",
    "from typing import Tuple, List\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "from workshop import data\n",
    "import helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop so far\n",
    "\n",
    "Going back to the last section of [**Notebook 4b**](4b_loss_functions_Solutions.ipynb), we finished by calculating the per batch loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nothing new here, just repeating definitions for clarity ... don't do this at home!\n",
    "\n",
    "class MnistDataSet:\n",
    "    def __init__(self, train=True):\n",
    "        subset = \"training\" if train else \"test\"\n",
    "        self.x, self.y = torch.load(data.DATA_PATH / f\"MNIST/processed/{subset}.pt\")\n",
    "        self.x = self.x.float()\n",
    "    \n",
    "    def __getitem__(self, key) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.x[key], self.y[key]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "class MnistDataLoader:\n",
    "    def __init__(self, dataset, batch_size, shuffle, transform=None):\n",
    "        self.dataset, self.batch_size, self.shuffle, self.transform = dataset, batch_size, shuffle, transform\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.idx = list(range(len(self.dataset)))\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.idx)\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.idx:\n",
    "            batch, self.idx = self.idx[:self.batch_size], self.idx[self.batch_size:]\n",
    "            x, y = self.dataset[batch]\n",
    "            if self.transform:\n",
    "                return self.transform(x, y)\n",
    "            return x, y\n",
    "        \n",
    "        raise StopIteration()\n",
    "\n",
    "class ImageNormalizer:\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean, self.std = mean, std\n",
    "    \n",
    "    def __call__(self, x, y):\n",
    "        return (x - self.mean).div_(self.std), y\n",
    "\n",
    "class Flatten(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.reshape(x.shape[0], -1)\n",
    "        \n",
    "\n",
    "def get_model():\n",
    "    return torch.nn.Sequential(collections.OrderedDict([\n",
    "        (\"reshape\", torch.nn.Flatten()),\n",
    "        (\"hidden\", torch.nn.Linear(28*28,256)),\n",
    "        (\"sigmoid\", torch.nn.Sigmoid()),\n",
    "        (\"output\", torch.nn.Linear(256,10)),\n",
    "      ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = MnistDataLoader(MnistDataSet(train=True), 1024, True, ImageNormalizer(33.32, 78.57))\n",
    "test_dl = MnistDataLoader(MnistDataSet(train=False), 1024, True, ImageNormalizer(33.32, 78.57))\n",
    "ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def accuracy(preds, target):\n",
    "    return (preds.max(-1)[1] == target).float().mean()\n",
    "\n",
    "# for x, y in train_dl:\n",
    "#     preds = model(x)\n",
    "#     print(ce(preds, y))\n",
    "#     print(accuracy(preds, y))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [**Notebook 2**](../2_Tensors/2b_Tensors_features_Solution.ipynb) we already saw such a loop when finding the parameters for a **linear fit** to some data by minimizing the **MSE**.\n",
    "\n",
    "Let's us try to reproduce this algorithm with our current setup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(model, n_epochs, lr):\n",
    "    train_losses = np.array([])\n",
    "    test_losses = np.array([])\n",
    "    accuracies = np.array([])\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for x, y in train_dl:\n",
    "            train_loss = ce(model(x), y)\n",
    "            train_loss.backward()\n",
    "            train_losses = np.append(train_losses, train_loss.item())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p += - lr * p.grad\n",
    "                    p.grad.data.zero_()\n",
    "\n",
    "        test_loss, acc = evaluate_nn(model, test_dl)\n",
    "        test_losses = np.append(test_losses, test_loss)\n",
    "        accuracies = np.append(accuracies, acc)\n",
    "\n",
    "        print(f\"Epoch: {epoch} \\t Training loss: {train_losses[-1]} \\t Test loss: {test_losses[-1]} \\t Test accurarcy: {accuracies[-1]}\")\n",
    "    return train_losses, test_losses, accuracies\n",
    "\n",
    "def evaluate_nn(model, test_dl):\n",
    "    preds = torch.tensor([])\n",
    "    targets = torch.tensor([]).long()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in test_dl:\n",
    "            targets = torch.cat([targets, y])\n",
    "            preds = torch.cat([preds, model(x)])\n",
    "        test_loss = ce(preds, targets)\n",
    "    return test_loss.item(), accuracy(preds, targets).item()\n",
    "\n",
    "def plot_metrics(train_losses, test_losses, accuracies):\n",
    "    fig, (ax0, ax1) = plt.subplots(1, 2, sharex=True, figsize=(15,5))\n",
    "    \n",
    "    x = np.array(range(len(train_losses)))\n",
    "    iterations_per_epoch = int(len(train_losses)/ len(test_losses))\n",
    "    x_val = x[iterations_per_epoch - 1 :: iterations_per_epoch]\n",
    "    ax0.plot(x, train_losses, label='train')\n",
    "    ax0.plot(x_val, test_losses, label='test')\n",
    "   \n",
    "    ax0.legend()\n",
    "    ax0.set_ylabel(\"Loss\")\n",
    "    ax0.set_xlabel(\"Iteration\")\n",
    "    \n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "    ax1.plot(x_val, accuracies)\n",
    "    ax1.set_xlabel(\"Iteration\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "train_losses, test_losses, accuracies = train_nn(model, n_epochs=10, lr=0.01)\n",
    "plot_metrics(train_losses, test_losses, accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Optimizers\n",
    "\n",
    "The above is a very neat example. The actual training code is only the part from l.6 to l.15, while the rest is mainly for logging.\n",
    "\n",
    "However, it looks like the part from l.12 to l.15 is very generic and it can certainly be refactored away:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, parameters, lr):\n",
    "        self.parameters, self.lr = list(parameters), lr\n",
    "    \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.parameters:\n",
    "                p += - self.lr * p.grad\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.parameters:\n",
    "                if p.grad is not None:\n",
    "                    p.grad.data.zero_()\n",
    "                \n",
    "                \n",
    "def train_nn(model, optim, n_epochs):\n",
    "    train_losses = np.array([])\n",
    "    test_losses = np.array([])\n",
    "    accuracies = np.array([])\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for x, y in train_dl:\n",
    "            \n",
    "            train_loss = ce(model(x), y)\n",
    "            train_loss.backward()\n",
    "            train_losses = np.append(train_losses, train_loss.item())\n",
    "\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "        test_loss, acc = evaluate_nn(model, test_dl)\n",
    "        test_losses = np.append(test_losses, test_loss)\n",
    "        accuracies = np.append(accuracies, acc)\n",
    "        \n",
    "        print(f\"Epoch: {epoch} \\t Training loss: {train_losses[-1]} \\t Test loss: {test_losses[-1]} \\t Test accurarcy: {accuracies[-1]}\")\n",
    "    return train_losses, test_losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "train_losses, test_losses, accuracies = train_nn(model, Optimizer(model.parameters(), lr=0.01), n_epochs=10)\n",
    "plot_metrics(train_losses, test_losses, accuracies)\n",
    "\n",
    "torch.save(model, \"../data/model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This basically introduces the core of what the [torch.optim.Optimizer](https://pytorch.org/docs/stable/_modules/torch/optim/optimizer.html#Optimizer) does. \n",
    "\n",
    "The `torch.optim` package, however, does not only offer an almost empty capsule for updating model parameters - it has all the deep learning batteries included: in our simple case, the weight update is performed with the **SGD** rule: \n",
    "$$\\omega_t = \\omega_{t-1} - \\lambda * \\nabla \\omega$$\n",
    "\n",
    "Much more advanced algorithms exist to perform the weight update, like **SGD with momentum**, **Adagrad**, **Adam**, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:\n",
    "\n",
    "Check the documentation for a couple of other optimizers and see if you can improve the model performance! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "train_losses, test_losses, accuracies = train_nn(model, torch.optim.AdamW(model.parameters(), lr=0.01), n_epochs=10)\n",
    "plot_metrics(train_losses, test_losses, accuracies)\n",
    "\n",
    "torch.save(model, \"../data/model_optimized_with_adam.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer(Optimizer):\n",
    "    def __init__(self, parameters, lr, beta1 = 0.9, beta2 = 0.999, epsilon=1e-8, wd=1e-2):\n",
    "        self.parameters, self.lr, self.beta1, self.beta2, self.epsilon, self.wd = list(parameters), lr, beta1, beta2, epsilon, wd\n",
    "        self.t, self.state = 0, [{\"m\": 0., \"v\": 0.} for _ in self.parameters]\n",
    "    \n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        with torch.no_grad():\n",
    "            for s, p in zip(self.state, self.parameters):\n",
    "                p.mul_(1-self.lr*self.wd)\n",
    "                s[\"m\"] = self.beta1 * s[\"m\"] + (1-self.beta1)*p.grad\n",
    "                s[\"v\"] = self.beta2 * s[\"v\"] + (1-self.beta2)*p.grad.pow(2)\n",
    "                p += - self.lr * math.sqrt(1-self.beta2**self.t) / (1 - self.beta1**self.t) * s[\"m\"] / (s[\"v\"].sqrt() + self.epsilon)\n",
    "                \n",
    "model = get_model()\n",
    "train_losses, test_losses, accuracies = train_nn(model, AdamOptimizer(model.parameters(), lr=0.01), n_epochs=10)\n",
    "plot_metrics(train_losses, test_losses, accuracies)\n",
    "\n",
    "torch.save(model, \"../data/model_optimized_with_adam.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambOptimizer(Optimizer):\n",
    "    def __init__(self, parameters, lr, beta1 = 0.9, beta2 = 0.999, epsilon=1e-8, wd=1e-2):\n",
    "        self.parameters, self.lr, self.beta1, self.beta2, self.epsilon, self.wd = list(parameters), lr, beta1, beta2, epsilon, wd\n",
    "        self.t, self.state = 0, [{\"m\": 0., \"v\": 0.} for _ in self.parameters]\n",
    "    \n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        with torch.no_grad():\n",
    "            for s, p in zip(self.state, self.parameters):\n",
    "                s[\"m\"] = self.beta1 * s[\"m\"] + (1-self.beta1)*p.grad\n",
    "                s[\"v\"] = self.beta2 * s[\"v\"] + (1-self.beta2)*p.grad.pow(2)\n",
    "                \n",
    "                r = s[\"m\"] / (s[\"v\"].sqrt() + self.epsilon)* math.sqrt(1-self.beta2**self.t) / (1 - self.beta1**self.t)\n",
    "                scale = r + self.wd*p\n",
    "                p += - self.lr * scale * torch.norm(p).clamp(0., 10.) / torch.norm(scale)\n",
    "                \n",
    "model = get_model()\n",
    "train_losses, test_losses, accuracies = train_nn(model, LambOptimizer(model.parameters(), lr=0.01), n_epochs=10)\n",
    "plot_metrics(train_losses, test_losses, accuracies)\n",
    "\n",
    "torch.save(model, \"../data/model_optimized_with_lamb.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2:\n",
    "\n",
    "What other API is exposed by the `torch.optim` package?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 2\n",
    "\n",
    "# Anwser is _LRScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 3:\n",
    "\n",
    "Save only the model's parameters to *../data/model_params.pt*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../data/model_params.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section summary\n",
    "\n",
    "The `torch.optim` package provides useful APIs and state-of-the-art algorithms for performing weight updates and learning rate scheduling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro_to_pytorch",
   "language": "python",
   "name": "intro_to_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
