{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4b. Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "from workshop import data\n",
    "import helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "So far, we have seen how \n",
    "\n",
    "1. the `torch.Tensor` and `autograd` framework enable the computation of gradients\n",
    "1. the `torch.nn module` helps us to define a neural network architecture\n",
    "1. the `torch.utils.data` dataset and a data loader high level APIs encapsulate provisioning of training examples\n",
    "\n",
    "The one step that is missing from training our own neural network now the computation of a **loss function**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A loss function for the MNIST dataset\n",
    "\n",
    "We have already seen a loss function in our Tensor notebook, namely **mean squared error**.\n",
    "\n",
    "This loss function, evaluating how **close we are to predicting the correct number** is an adequate choice for a regression problem. But for training digit classification we rather want to evaluate how **close we are to predicting the correct class**.\n",
    "\n",
    "The latter is typically achieved using the **cross entropy loss** function, which (in our case) measures the **negative log likelihood** for the target class $t$:\n",
    "$$ce(\\hat p) = -\\log(p_t)$$\n",
    "\n",
    "Here, $\\hat p$ is the vector of predicted probabilities for each class and $p_t$ is the predicted probability for the target class.\n",
    "\n",
    "Notice that previously we have purposefully defined our neural network to output a **softmax layer**, which can be interpreted as $\\hat p$.\n",
    "With that convention in mind, we can easliy define our loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(predictions: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    # predictions: probabilities of shape [batch_size, n_classes]\n",
    "    # targets: values {0, n_classes - 1} of shape [batch_size]\n",
    "    \n",
    "    return -predictions[range(len(target)), target].log().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to notice:\n",
    "\n",
    "* We use pairwise indexing here, which we have seen before\n",
    "* This works, because our target vector is a 1d vector of class labels that correspond to indices into our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor([0, 1, 2])\n",
    "\n",
    "perfect_preds = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])\n",
    "torch.testing.assert_allclose(nll(perfect_preds, targets), 0)\n",
    "\n",
    "bad_preds = torch.tensor([[0., 1., 0.], [1., 0., 0.], [1., 0., 0.]])\n",
    "torch.testing.assert_allclose(nll(bad_preds, targets), np.inf)\n",
    "\n",
    "some_preds = torch.tensor([[.98, 0.01, 0.01], [0.5, .5, 0.], [0.4, 0.4, .2]])\n",
    "torch.testing.assert_allclose(nll(some_preds, targets), -(np.log(.98) + np.log(.5) + np.log(0.2))/3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions in PyTorch\n",
    "\n",
    "There are a couple of reasons why one might not want to use our custom implementation of the nll loss but prefer to choose [the PyTroch implementation](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html?highlight=nllloss#torch.nn.NLLLoss).\n",
    "\n",
    "The documentation for `torch.nn.NLLLoss` explains that it consumes log probabilities. The advantage of this approach is that it provides more numerical stability by using the [log-sum-exp](https://en.wikipedia.org/wiki/LogSumExp) trick.\n",
    "\n",
    "It also outlines a different approach that avoids using a softmax layer in the model altogether, by means of using [Cross Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#crossentropyloss).\n",
    "\n",
    "## Bottom line\n",
    "PyTorch provides a **large number of loss functions** that are applicable across a wide range of deep learning tasks and that handle caveats like the one described above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
