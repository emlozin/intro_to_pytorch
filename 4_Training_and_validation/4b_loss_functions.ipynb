{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "from typing import Tuple, List\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "from workshop import data\n",
    "import helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4b Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "So far, we have seen how \n",
    "\n",
    "1. the torch.Tensor and autograd framework enable the computation of gradients\n",
    "1. the torch.nn module helps us to define a neural network architecture\n",
    "1. the torch.utils.data dataset and a data loader high level APIs encapsulate provisioning of training examples\n",
    "\n",
    "The one step that is missing from training our own neural network now the computation of a **loss function**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A loss function for the MNIST dataset\n",
    "\n",
    "We have already seen a loss function in our Tensor notebook, namely **mean squared error**.\n",
    "\n",
    "This loss function, evaluating how **close we are to predicting the correct number** is an adequate choice for a regression problem. But for training digit classification we rather want to evaluate how **close we are to predicting the correct class**.\n",
    "\n",
    "The latter is typically achieved using the **cross entropy loss** function, which (in our case) measures the **negative log likelihood** for the target class $t$:\n",
    "$$ce(\\hat p) = -\\log(p_t)$$\n",
    "\n",
    "Here, $\\hat p$ is the vector of predicted probabilities for each class and $p_t$ is the predicted probability for the target class.\n",
    "\n",
    "Notice that previously we have purposefully defined our neural network to output a **softmax layer**, which can be interpreted as $\\hat p$.\n",
    "With that convention in mind, we can easliy define our loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(predictions: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    # predictions: probabilities of shape [batch_size, n_classes]\n",
    "    # targets: values {0, n_classes - 1} of shape [batch_size]\n",
    "    \n",
    "    return -predictions[range(len(target)), target].log().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to notice:\n",
    "\n",
    "* We use pairwise indexing here, which we have seen before\n",
    "* This works, because our target vector is a 1d vector of class labels that correspond to indices into our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor([0, 1, 2])\n",
    "\n",
    "perfect_preds = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])\n",
    "torch.testing.assert_allclose(nll(perfect_preds, targets), 0)\n",
    "\n",
    "bad_preds = torch.tensor([[0., 1., 0.], [1., 0., 0.], [1., 0., 0.]])\n",
    "torch.testing.assert_allclose(nll(bad_preds, targets), np.inf)\n",
    "\n",
    "some_preds = torch.tensor([[.98, 0.01, 0.01], [0.5, .5, 0.], [0.4, 0.4, .2]])\n",
    "torch.testing.assert_allclose(nll(some_preds, targets), -(np.log(.98) + np.log(.5) + np.log(0.2))/3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions in PyTorch\n",
    "\n",
    "There are a couple of reasons why one might not want to use our custom implementation of the nll loss but prefer to choose [the PyTroch implementation](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html?highlight=nllloss#torch.nn.NLLLoss).\n",
    "\n",
    "The documentation for `torch.nn.NLLLoss` explains that it consumes log probabilities. The advantage of this approach is that it proveides more numerical stability by using the [log-sum-exp](https://en.wikipedia.org/wiki/LogSumExp) trick.\n",
    "\n",
    "It also outlines a different approach that avoids using a softmax layer in the model altogether, by means of using [Cross Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#crossentropyloss).\n",
    "\n",
    "## Bottom line\n",
    "PyTorch provides a **large number of loss functions** that are applicable across a wide range of deep learning tasks and that handle caveats like the one described above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Using the Sequential model from 3b and our dataset and loader from 4a, calculate the batchwise cross entropy loss.\n",
    "\n",
    "**Notice:** At this time, our dataloader outputs a tensor of shape [batch size, 28, 28]. The linear layer expects something of the shape [batch size, 28x28], so we have to reshape somewhere. One idea would be to define an extra `Flatten(torch.nn.Module)` layer and prepend it to our existing sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDataSet:\n",
    "    def __init__(self, train=True):\n",
    "        subset = \"training\" if train else \"test\"\n",
    "        self.x, self.y = torch.load(data.DATA_PATH / f\"MNIST/processed/{subset}.pt\")\n",
    "        self.x = self.x.float()\n",
    "    \n",
    "    def __getitem__(self, key) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.x[key], self.y[key]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "class MnistDataLoader:\n",
    "    def __init__(self, dataset, batch_size, shuffle, transform=None):\n",
    "        self.dataset, self.batch_size, self.shuffle, self.transform = dataset, batch_size, shuffle, transform\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.idx = list(range(len(self.dataset)))\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.idx)\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.idx:\n",
    "            batch, self.idx = self.idx[:self.batch_size], self.idx[self.batch_size:]\n",
    "            x, y = self.dataset[batch]\n",
    "            if self.transform:\n",
    "                return self.transform(x, y)\n",
    "            return x, y\n",
    "        \n",
    "        raise StopIteration()\n",
    "\n",
    "class ImageNormalizer:\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean, self.std = mean, std\n",
    "    \n",
    "    def __call__(self, x, y):\n",
    "        return (x - self.mean).div_(self.std), y\n",
    "\n",
    "class Flatten(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.reshape(x.shape[0], -1)\n",
    "        \n",
    "model = torch.nn.Sequential(collections.OrderedDict([\n",
    "            (\"reshpae\", Flatten()),\n",
    "            (\"hidden\", torch.nn.Linear(28*28,256)),\n",
    "            (\"sigmoid\", torch.nn.Sigmoid()),\n",
    "            (\"output\", torch.nn.Linear(256,10)),\n",
    "          ]))\n",
    "\n",
    "def accurcary(preds, target):\n",
    "    return (preds.max(-1)[1] == y).float().mean()\n",
    "\n",
    "train_dl = ??\n",
    "\n",
    "for x, y in train_dl:\n",
    "    # TODO: Calculate the cross-entropy loss for each batch\n",
    "    pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro_to_pytorch",
   "language": "python",
   "name": "intro_to_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
